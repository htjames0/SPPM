{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8facda1",
   "metadata": {},
   "source": [
    "# Simple Stock Price Prediction Model\n",
    "Here I will be implementing a simple MLP neural network for stock price prediction.  My GitHub profile is htjames0 and all code and files can be found in the repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee21751d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc66348d",
   "metadata": {},
   "source": [
    "## Data Methods\n",
    "\n",
    "### Introduction\n",
    "The data methods will be used to convert the data from the preparsed test and training data into useable and clean training and test data that will be fed to the model.  The first method is the getTrainData method which will be used to convert the training data into the appropriate scale, size, and format.  The second method, getTestData, will be used to get the test data in order to see how accuarate the model will predict the price of a stock.  The test data will not be used until all training parameters are tuned and the model is fully optimized on the training data.  Hyperparameter tunign includes finding the optimal stepsize, activation function, hidden number of neurons, in addition to the optimal window (i.e. 15, 30, 45, 90, etc.). Each method takes the same parameters, data and windows.  The data parameter is self explainatory, if using the getTestData then you input the test_data and visa versa for the getTrainData. The window parameter is an integer that is used to determine how many days to look back in time in order to predict the price of the stock for the next day.  This parameter might be the most confusing but is important as the training data is structured based on this parameter. \n",
    "\n",
    "#### Example 1. Understanding the Window:\n",
    "> For example, suppse the window is 15 days and we have training data from Jan 1st, 2000 to January 1st, 2002.  The window will start at January 16th, 2000 and increment forward in time by one day every time.  For this iteration, the the features (Open, High, Low and Volume) will be appended into an array with the past 15 days of data.  Also the label (Closing Price) will be recorded for January 16th.  So for the first iteration the data will look something like this, \n",
    "\n",
    "\n",
    "> We see that the closing price and feature data is recorded.  The second iteration will look like this, \n",
    "\n",
    "\n",
    "> This process will continue from January 16th, 2000 until January 1st, 2002 in this example.  Each time incrementing by one day and looking back into the past to find the correct 15 days worth of data. This will iterate the number of days of the training data set minus window number of days.  Here there are roughly 504 trading days (252 in a year) with a window of 15 thus, it would iterate 489 times to collect all the data.  After this is done the data will be reshaped into a 3D array in which each iteration will be one layer.\n",
    "\n",
    "The data will be structured into a 3D array where the row will be the length of the window, columns will be the number of features, and layers will be the number of iterations to obtain the correct data. One can visulaize this as stacking each successive iteration behind the previous one to form a oblong rubix cube shape.  Another way to visulize it is a deck of baseball cards where each iteration is one card, the length of the card is the number of rows, and the width of the card is the number of features (Figure 1).  The top card is the most distant timeframe of data and the bottom card is the most recent.  \n",
    "\n",
    "#### Example 1 cont. \n",
    "> So continuing the example fro above, the rows will be the number of days into the past, 15 days in this example.  The columns being the features (Open, High, Low, and Volume).  And the layers represent each iteration of finding data for the correct windows.\n",
    "\n",
    "\n",
    "### Training Data Method\n",
    "Below is the implementation of the getTrainData method.  There are comments in the code the explain how the code works in order to achieve the explaination above.  The method takes in the prepartitioned training data and the window length to return the x_train and y_train data for the model to be fed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8cacf1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainData(data, window=15):\n",
    "    #itializing feature and label arrays\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "\n",
    "    #iterating from the start of the window to the length of the data \n",
    "    #to get a window of days for data points and making the label that last\n",
    "    #day of that specific window\n",
    "    for i in range(window, len(data)):\n",
    "        x_train.append(data.iloc[i-window:i,[1,2,3,6]])  #the indexing here is specific to the AMD dataset\n",
    "        y_train.append(data.iloc[i,4])\n",
    "    \n",
    "    #converting to numpy array     \n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    #3D array in python - (layer, row, column)\n",
    "    #layer is each iteration of the loop, 1169\n",
    "    #row is number of days used to predict next day, 90\n",
    "    #column is the feature, Open, High, Low, or Volume \n",
    "    #4 is a bit hardcoded here and could be changed  to be an input variable of the method\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 4))\n",
    "    \n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85abb91f",
   "metadata": {},
   "source": [
    "### Test Data Method: \n",
    "The implementation of the Test Data set is quite similar to the getTrainData method above. The method takes parameters data and window to return the x_test and y_test data for the model to be tested with.  This data will be used to test how accurate the model is in predicting the stock price of the next day.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "68627efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to think about how the test data is being generated \n",
    "    #how the window plays into effect here...need to feed it 90 days worth of training data for \n",
    "    #the first iteration of test data generation\n",
    "    \n",
    "def getTestData(data, window=90):\n",
    "    #initializing arrays\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    \n",
    "    #iterating over the test data to gather the appropriate features \n",
    "    #for the correct label\n",
    "    for i in range(window, len(data)):\n",
    "            x_test.append(data.iloc[i-window:i,[1,2,3,6]])\n",
    "            y_test.append(data.iloc[i,4]) \n",
    "        \n",
    "    #reshaping the data into a 3D array format\n",
    "    #changing formate of arrays\n",
    "    x_test = np.array(x_test)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    #changing array shape \n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_train.shape[1], 4))\n",
    "    \n",
    "    return x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186adaa8",
   "metadata": {},
   "source": [
    "## The Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5cbc6f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelMLP(features, step_size, hidden_neurons=5, act='relu', loss_fxn='mean_squared_error'):\n",
    "    \n",
    "    #method inputs\n",
    "        #features - feature data used to get the number of input layer nodes\n",
    "        #step-size - for optimizer\n",
    "        #hidden_neurons - number of neurons in hidden layer, default of five\n",
    "        #act - activation function, default ReLu function\n",
    "        #loss_fxn - loss function, default as mean squared error\n",
    "    \n",
    "    #defining model\n",
    "    model = tf.keras.models.Sequential()\n",
    "   \n",
    "    #layers\n",
    "    model.add(tf.keras.layers.Dense(units=hidden_neurons,\n",
    "                                    input_shape=(features.shape[1],1),\n",
    "                                    activation='relu')\n",
    "                                   )\n",
    "    model.add(tf.keras.layers.Dense(units = 1,\n",
    "                                    activation=act)\n",
    "                                   )\n",
    "    \n",
    "    #optimizer\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=step_size)\n",
    "    \n",
    "    #compile - loss fxn MSE\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss=loss_fxn,\n",
    "                  metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "   \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2bc9a19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train function\n",
    "def modelTrain(model, feature, label, epochs, batch_size):\n",
    "\n",
    "    #feeding features and labels to model, model iterates epoch number of times\n",
    "    #using batch_size number of data points per iteration\n",
    "    history = model.fit(x=feature,\n",
    "                        y=label,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs)\n",
    "\n",
    "    #weights and bias\n",
    "    trained_weight = model.get_weights()[0]\n",
    "    trained_bias = model.get_bias()[1]\n",
    "\n",
    "    #historical data of model for each epoch\n",
    "    epochs = history.epoch\n",
    "    hist = pd.DateFram(history.history)\n",
    "\n",
    "    #rmse for each epoch\n",
    "    rmse = hist[\"root_mean_squared_error\"]\n",
    "\n",
    "    return trained_weight, trained_bias, epochs, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "013ddb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test function\n",
    "def modelTest(model, feature, label): \n",
    "    loss = model.evaluate(feature, label, verbose=0)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ded1de4",
   "metadata": {},
   "source": [
    "## Execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "48a2d09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining constants \n",
    "window = 90\n",
    "\n",
    "#importing data\n",
    "data = pd.read_csv('AMD.csv')\n",
    "data = data.round(3)\n",
    "data['Volume'] = data['Volume']/1000000\n",
    "\n",
    "#dividing the data set into train and test data\n",
    "train_end = data[data.Date=='2021-01-04'].index[0]\n",
    "train_data = data.iloc[:train_end,:]\n",
    "test_data = data.iloc[train_end - window:,:]\n",
    "\n",
    "#calling the Data Methods to get the appropriate data \n",
    "x_train, y_train = getTrainData(train_data, window=90)\n",
    "x_test, y_test = getTestData(test_data, window=90)\n",
    "\n",
    "#creating model\n",
    "model = modelMLP(x_train, step_size=0.01, hidden_neurons=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
